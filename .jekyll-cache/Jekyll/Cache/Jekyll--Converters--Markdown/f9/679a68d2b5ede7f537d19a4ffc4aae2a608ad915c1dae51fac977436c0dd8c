I"—<p>I collected with web scraping 22000 category labeled articles from multiple Romanian news sites and did data exploration and text classification.</p>

<p>The dataset contains news articles with columns: <em>URL</em>, <em>source</em>, <em>date</em>, <em>category</em>, <em>title</em>, <em>content</em>.</p>

<p>There are articles published from November 2016 up to October 2021 in the dataset, however most of them are from 2020 and 2021.</p>

<p>The articles are evenly split among 5 categories to avoid imbalanced classifications (4400 per class): politics, social, economy, science, sports.</p>

<p>The articles are collected from 5 news sites: <em>Digi24</em>, <em>Mediafax</em>, <em>Libertatea</em>, <em>News.ro</em>, <em>RFI</em>.</p>

<iframe src="/img/posts/news/fig_donut_1.html" height="500px" width="100%" frameborder="0" scrolling="no"></iframe>

<p>I did web scraping using the <a href="https://docs.python-requests.org/">requests</a> and <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">BeautifulSoup</a> libraries in Python. I used a timeout between requests to not overload the servers.</p>

<p>Most frequent words can be visualised with a word cloud.</p>

<p><img src="/img/posts/news/fig_wordcloud_1.png" /></p>

<p>We can also break it down per categories with bar charts.</p>

<iframe src="/img/posts/news/fig_bar_1.html" height="600px" width="100%" frameborder="0" scrolling="no"></iframe>
<iframe src="/img/posts/news/fig_bar_2.html" height="600px" width="100%" frameborder="0" scrolling="no"></iframe>
<iframe src="/img/posts/news/fig_bar_3.html" height="600px" width="100%" frameborder="0" scrolling="no"></iframe>
<iframe src="/img/posts/news/fig_bar_4.html" height="600px" width="100%" frameborder="0" scrolling="no"></iframe>
<iframe src="/img/posts/news/fig_bar_5.html" height="600px" width="100%" frameborder="0" scrolling="no"></iframe>

<p>In order to train a machine learning model, it is necessary to preprocess the text first. I removed punctuation, special characters, digits, lowercased and lemmatized the words. Then I vectorized the text with <a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">TF-IDF</a>.</p>

<p>If we use a dimensionality reduction algorithm, such as <a href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html">t-SNE</a>, we can visualize the news articles. The ones that are close to each other are supposed to be more related. I recommend opening the scatter plot below in <a href="/img/posts/news/fig_scatter_1.html">full screen view</a> and hovering the cursor over articles for more information.</p>

<iframe src="/img/posts/news/fig_scatter_1.html" height="800px" width="100%" frameborder="0" scrolling="no"></iframe>

<p>I tested multiple machine learning classifiers. I did a grid search with 10-fold cross validation for hyperparameter tuning. <a href="9https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html">SGD</a> was the best classifier found at 86.91% cross validation accuracy. An <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html">ensemble voting</a> of all the classifiers combined got 86.90% accuracy, tied with <a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html">linear SVC</a>. See table below.</p>

<table>
  <thead>
    <tr>
      <th>Classifier</th>
      <th>Accuracy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><a href="9https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html">SGD</a></td>
      <td>86.91%</td>
    </tr>
    <tr>
      <td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html">Voting</a></td>
      <td>86.90%</td>
    </tr>
    <tr>
      <td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html">Linear SVC</a></td>
      <td>86.90%</td>
    </tr>
    <tr>
      <td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">Logistic Regression</a></td>
      <td>86.61%</td>
    </tr>
    <tr>
      <td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html">Multinomial NB</a></td>
      <td>85.83%</td>
    </tr>
    <tr>
      <td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html">Extra Trees</a></td>
      <td>84.99%</td>
    </tr>
    <tr>
      <td><a href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html">K Neighbors</a></td>
      <td>84.18%</td>
    </tr>
  </tbody>
</table>

<p>Source code is here.</p>
:ET