---
layout: post
title: "News Articles Classification"
subtitle: "Web scraping and machine learning text classification"
background: '/img/posts/news/newspaper.jpg'
---

I collected with web scraping 22000 category labeled articles from multiple Romanian news sites and did data exploration and text classification. 

The dataset contains news articles with columns: *URL*, *source*, *date*, *category*, *title*, *content*. 

There are articles published from November 2016 up to October 2021 in the dataset, however most of them are from 2020 and 2021. 

The articles are evenly split among 5 categories to avoid imbalanced classifications (4400 per class): politics, social, economy, science, sports.

The articles are collected from 5 news sites: *Digi24*, *Mediafax*, *Libertatea*, *News.ro*, *RFI*.

<iframe src="/img/posts/news/fig_donut_1.html" height="500px" width="100%" frameBorder="0" scrolling="no"></iframe>

I did web scraping using the [requests](https://docs.python-requests.org/) and [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) libraries in Python. I used a timeout between requests to not overload the servers.

Most frequent words can be visualised with a word cloud.

<img src="/img/posts/news/fig_wordcloud_1.png"/>

We can also break it down per categories with bar charts.

<iframe src="/img/posts/news/fig_bar_1.html" height="600px" width="100%" frameBorder="0" scrolling="no"></iframe>
<iframe src="/img/posts/news/fig_bar_2.html" height="600px" width="100%" frameBorder="0" scrolling="no"></iframe>
<iframe src="/img/posts/news/fig_bar_3.html" height="600px" width="100%" frameBorder="0" scrolling="no"></iframe>
<iframe src="/img/posts/news/fig_bar_4.html" height="600px" width="100%" frameBorder="0" scrolling="no"></iframe>
<iframe src="/img/posts/news/fig_bar_5.html" height="600px" width="100%" frameBorder="0" scrolling="no"></iframe>

In order to train a machine learning model, it is necessary to preprocess the text first. I removed punctuation, special characters, digits, lowercased and lemmatized the words. Then I vectorized the text with [TF-IDF](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).

If we use a dimensionality reduction algorithm, such as [t-SNE](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html), we can visualize the news articles. The ones that are close to each other are supposed to be more related. I recommend opening the scatter plot below in [full screen view](/img/posts/news/fig_scatter_1.html) and hovering the cursor over articles for more information.

<iframe src="/img/posts/news/fig_scatter_1.html" height="800px" width="100%" frameBorder="0" scrolling="no"></iframe>

I tested multiple machine learning classifiers. I did a grid search with 10-fold cross validation for hyperparameter tuning. [SGD](9https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) was the best classifier found at 86.91% cross validation accuracy. An [ensemble voting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html) of all the classifiers combined got 86.90% accuracy, tied with [linear SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html). See table below.

| Classifier                                                                                                            | Accuracy |
|-----------------------------------------------------------------------------------------------------------------------|----------|
| [SGD](9https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)                     | 86.91%   |
| [Voting](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)                    | 86.90%   |
| [Linear SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)                            | 86.90%   |
| [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) | 86.61%   |
| [Multinomial NB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)            | 85.83%   |
| [Extra Trees](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)           | 84.99%   |
| [K Neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)          | 84.18%   |

Source code is [here](https://github.com/EdwardLiv/News-Articles).